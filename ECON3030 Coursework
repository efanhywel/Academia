{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Import the PWT 10.1 to Python.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sci\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/jivizcaino/PWT10.1/main/pwt101.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Choose a year to perform your development accounting analysis. The year chosen should be the latest year available that maximizes the number of observations for the variables of interest. Justify\n",
    "your choice by providing a table with descriptive statistics for each variable in the year chosen.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restricts to years after 2010, and for the variables of interest\n",
    "df_range = df[(df['year'] >= 2010)]\n",
    "df_range_var = df_range[['year', 'rgdpna', 'emp', 'avh', 'hc', 'labsh', 'rtfpna', 'rnna']]\n",
    "\n",
    "#Gets unique years from 'year'\n",
    "unique_yr = df_range_var['year'].unique()\n",
    "\n",
    "#Starts for loop for each unique year\n",
    "for year in unique_yr:\n",
    "    df_unique_yr = df_range_var[df_range_var['year'] == year] #Creates new dataframe with only variable observations from specified year\n",
    "\n",
    "    print(f\"\\nYear: {year}\") #Uses f so that variable values can be printed in text\n",
    "    total_var_count = 0  #Initialises total_var_count to 0\n",
    "\n",
    "    for var in df_unique_yr.columns[1:]: #For loop in df_unique_yr; starts at 1 because the first column is year\n",
    "        var_count = df_unique_yr[var].count()\n",
    "        print(f\"{var}: {var_count} observations\")\n",
    "        total_var_count += var_count  #Add the count of each variable to the total\n",
    "\n",
    "    print(f\"Total: {total_var_count} observations\") #Prints total observations for each year; in main for loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I selected the year 2019, as all years 2010-2019 have either 1006 or 1007 total observations for the variables of interest; thus, the recency of the data is the main concern. This code prints summary statistics for the selected variables for 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2019 = df[df['year'] == 2019]\n",
    "df_2019_interest = df_2019[['country','rgdpna', 'emp', 'avh', 'hc', 'labsh', 'rtfpna', 'rnna']] #Selects the variables of interest\n",
    "print(df_2019_interest.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Provide a table of descriptive statistics characterizing differences in income per worker in your sample. You should also include the richest and the poorest countries, the countries in the 90th and 10th percentiles, the countries in the 95th and 5th percentiles, and the variance of GDP per worker. Compute the gap between the richest and the poorest countries in your sample, and those in the percentiles 90th and 10th, 95th and 5th. Tabulate your results accordingly.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates income per worker for 2019\n",
    "gdppw = df_2019['rgdpna'] / df_2019['emp']\n",
    "\n",
    "#Finds country with highest gdppw\n",
    "max_loc = gdppw.idxmax() #Finds the location of the maximum income per worker\n",
    "max_country_name = df.loc[max_loc, 'country'] #Locates which country has the maximum gdppw\n",
    "max_value = gdppw.max() #Finds the value of the maximum income per worker\n",
    "max_value_fin = round(max_value, 2) #Rounds to two decimal points\n",
    "\n",
    "#Finds country with lowest gdppw\n",
    "min_country = gdppw.idxmin()\n",
    "min_country_name = df_2019.loc[min_country, 'country']\n",
    "min_value = gdppw.min()\n",
    "min_value_fin = round(min_value, 2)\n",
    "\n",
    "#Finds country at the 90th percentile of gdppw\n",
    "prc_90 = gdppw.quantile(0.9) #Calculates 90th percentile of gdppw\n",
    "closest_prc_90 = min(gdppw, #Finds the value of the closest gdppw to the 90th percentile (absolute value)\n",
    "key = lambda #Lambda creates quick function without actually defining it\n",
    "x:abs(x-prc_90)) \n",
    "closest_prc_90_fin = round(closest_prc_90, 2)\n",
    "closest_prc_90_index = gdppw.sub(prc_90).abs().idxmin() #Finds the index of the closest gdppw to the 90th percentile\n",
    "country_closest_prc_90 = df_2019.loc[closest_prc_90_index, 'country'] #Finds the name of the country closest to the 90th percentile\n",
    "\n",
    "#Finds country at 10th percentile of gdppw\n",
    "prc_10 = gdppw.quantile(0.1)\n",
    "closest_prc_10 = min(gdppw, key = lambda x:abs(x-prc_10))\n",
    "closest_prc_10_fin = round(closest_prc_10, 2)\n",
    "closest_prc_10_index = gdppw.sub(prc_10).abs().idxmin()\n",
    "country_closest_prc_10 = df_2019.loc[closest_prc_10_index, 'country']\n",
    "\n",
    "#Finds country at 95th percentile of gdppw\n",
    "prc_95 = gdppw.quantile(0.95)\n",
    "closest_prc_95 = min(gdppw, key = lambda x:abs(x-prc_95))\n",
    "closest_prc_95_fin = round(closest_prc_95, 2)\n",
    "closest_prc_95_index = gdppw.sub(prc_95).abs().idxmin()\n",
    "country_closest_prc_95 = df_2019.loc[closest_prc_95_index, 'country']\n",
    "\n",
    "#Finds country at 5th percentile of gdppw\n",
    "prc_05 = gdppw.quantile(0.05)\n",
    "closest_prc_05 = min(gdppw, key = lambda x:abs(x-prc_05))\n",
    "closest_prc_05_fin = round(closest_prc_05, 2)\n",
    "closest_prc_05_index = gdppw.sub(prc_05).abs().idxmin()\n",
    "country_closest_prc_05 = df_2019.loc[closest_prc_05_index, 'country']\n",
    "\n",
    "#Calculates variance in gdppw\n",
    "gdppw_variance = np.var(gdppw)\n",
    "\n",
    "#Calculates difference in gdppw between richest and poorest country\n",
    "diff_rich_poor = max_value - min_value\n",
    "diff_rich_poor_fin = round(diff_rich_poor, 2)\n",
    "\n",
    "#Calculates difference in gdppw between 90th and 10th percentile\n",
    "diff_90_10 = closest_prc_90 - closest_prc_10\n",
    "diff_90_10_fin = round(diff_90_10, 2)\n",
    "\n",
    "#Calculates difference in gdppw between 95th and 5th percentile\n",
    "diff_95_05 = closest_prc_95 - closest_prc_05\n",
    "diff_95_05_fin = round(diff_95_05, 2)\n",
    "\n",
    "#Prints descriptive statistics about income per worker\n",
    "print(\"Descriptive statistics about income per worker:\")\n",
    "print(gdppw.describe())\n",
    "\n",
    "#Create dictionary with calculated data\n",
    "gdppw_questions = {\"Metric\": ['Maximum GDP per Worker', 'Minimum GDP per Worker', 'Country at 90th Percentile of GDP per Worker', 'Country at 10th Percentile of GDP per Worker',\n",
    "                              'Country at 95th Percentile of GDP per Worker', 'Country at 5th Percentile of GDP per Worker', 'Variance in GDP per Worker', \n",
    "                              'Difference in GDP per Worker (Richest - Poorest)', 'Difference in GDP per Worker (90th - 10th Percentile)', 'Difference in GDP per Worker (95th - 5th Percentile)',],\n",
    "                   \"Value\": [max_value_fin, min_value_fin, closest_prc_90_fin, closest_prc_10_fin, closest_prc_95_fin, closest_prc_05_fin, gdppw_variance, diff_rich_poor_fin,\n",
    "                            diff_90_10_fin, diff_95_05_fin],\n",
    "                   \"Country\": [max_country_name, min_country_name, country_closest_prc_90, country_closest_prc_10, country_closest_prc_95, country_closest_prc_05, \"N/A\", \n",
    "                              f'{max_country_name} - {min_country_name}', f'{country_closest_prc_90} - {country_closest_prc_10}', f'{country_closest_prc_95} - {country_closest_prc_05}',]}\n",
    "\n",
    "#Create and print a dataframe\n",
    "gdppw_table = pd.DataFrame(gdppw_questions)\n",
    "print()\n",
    "print(\"Table of findings:\")\n",
    "print(gdppw_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Produce scatterplots for ln (ùëò), ln (‚Ñé), ùëéùë£‚Ñé, (1 ‚àí ùõº) (using labsh) and ln (ùê¥) (using rtfpna) against the natural logarithm of GDP per worker, compute as ln(ùë¶) = ln (ùëå/ùê∏). Label the axes, accordingly, add a regression line and a regression equation to each chart, and label the countries in the percentiles 5th,10th,25th,50th,75th, 90th and 95th using the variable countrycode. According to your charts, which variables show the highest variability across the development spectrum?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. (i): ln(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatterplot-------------------------------\n",
    "from scipy.stats import percentileofscore #Allows for labelling of percentiles\n",
    "\n",
    "#Calculates capital per worker, k\n",
    "cppw = df_2019['rnna'] / df_2019['emp']\n",
    "\n",
    "#Calculates ln(k) and ln(y)\n",
    "df_2019['ln_k'] = np.log(cppw)\n",
    "df_2019['ln_y'] = np.log(gdppw)\n",
    "\n",
    "#Drops NaN values (important)\n",
    "df_2019_ln_k = df_2019.dropna(subset = ['ln_y', 'ln_k'])\n",
    "\n",
    "#Produces scatterplot\n",
    "plt.scatter(df_2019_ln_k['ln_y'], df_2019_ln_k['ln_k'])\n",
    "plt.xlabel(\"ln(y) (natural logarithm of income per worker)\") #Labels axes\n",
    "plt.ylabel(\"ln(k) (natural logarithm of physical capital per worker)\")\n",
    "plt.title(\"Scatterplot for ln(k) against ln(y)\") #Adds title\n",
    "\n",
    "#Regression line-----------------------------\n",
    "#Calculates coefficients of regression line (polynomial of degree 1)\n",
    "reg_k_coefficients = np.polyfit(df_2019_ln_k['ln_y'], df_2019_ln_k['ln_k'], 1)\n",
    "\n",
    "x_vals = np.linspace(df_2019_ln_k['ln_y'].min(), df_2019_ln_k['ln_y'].max(), 100) #Creates 100 values as x-coordinates\n",
    "y_vals = reg_k_coefficients[0] * x_vals + reg_k_coefficients[1] #Calculates corresponding y values for each x-coordinate using reg_k_coefficients\n",
    "\n",
    "sign = \"+\" if reg_k_coefficients[1] >= 0 else \"-\" #Accesses intercept term of reg_k_coefficients, and is + if it's positive, and - if it's negative\n",
    "plt.plot(x_vals, y_vals, color = 'red', #Plots red regression line\n",
    "label = f\"ln(k) = {reg_k_coefficients[0]:.2f}ln(y) {sign} {abs(reg_k_coefficients[1]):.2f}\") #Includes coefficients to 2 d.p, and takes absolute value of intercept so that {sign} works properly\n",
    "\n",
    "#Percentiles--------------------------------\n",
    "percentiles = [5, 10, 25, 50, 75, 90, 95] #Creates percentiles of interest\n",
    "for percentile in percentiles: #Loop iterating for each percentile in the percentiles list (i.e. percentiles of interest)\n",
    "    ln_k_percentile_value = np.percentile(df_2019_ln_k['ln_k'], percentile) #Calculates the value of ln(k) for each percentile of interest\n",
    "    \n",
    "    #Finds the country with the ln_k value closest to the percentile value\n",
    "    closest_country_idx = np.abs(df_2019_ln_k['ln_k'] - ln_k_percentile_value).idxmin() #Takes absolute difference between each country's ln(k) value and the value of ln(k) at each percentile\n",
    "    country_at_percentile = df_2019_ln_k.loc[closest_country_idx, 'countrycode'] #Finds the country with the lowest absolute difference between its ln(k) and percentile ln(k) value.\n",
    "    \n",
    "    plt.text(df_2019_ln_k.loc[df_2019_ln_k['countrycode'] == country_at_percentile, 'ln_y'], #Finds the countrycode that matches with the country closest to the percentile and finds its ln(y)\n",
    "             df_2019_ln_k.loc[df_2019_ln_k['countrycode'] == country_at_percentile, 'ln_k'], #Does the same but for ln(k)\n",
    "             f'{country_at_percentile}\\n{percentile}th', #/n causes line break\n",
    "             fontsize = 9, ha = 'right') #Aligns text to the right\n",
    "    plt.annotate(\"\",  #Empty string means that annotation does not contain any text - just arrow\n",
    "                 xy = (df_2019_ln_k.loc[df_2019_ln_k['countrycode'] == country_at_percentile, 'ln_y'].values[0], #x value is ln(y) value\n",
    "                     df_2019_ln_k.loc[df_2019_ln_k['countrycode'] == country_at_percentile, 'ln_k'].values[0]), #y value is ln(k) value\n",
    "                 xytext = (df_2019_ln_k.loc[df_2019_ln_k['countrycode'] == country_at_percentile, 'ln_y'].values[0] + 0.3, #Offsets arrow tail by 0.3 \n",
    "                                                                                                                           #on the x-axis (though still points to correct coord)\n",
    "                         df_2019_ln_k.loc[df_2019_ln_k['countrycode'] == country_at_percentile, 'ln_k'].values[0] + 0.2),\n",
    "                 arrowprops = dict(color = 'black', arrowstyle = '->'))\n",
    "\n",
    "#Final------------------------------------\n",
    "plt.legend()\n",
    "plt.tight_layout() #Changes axis values to fit diagram\n",
    "plt.show() #Prints diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there is a clear outlier in this data, I will use the IQR technique to get rid of outliers, and present both the original and no outlier versions for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outliers-----------------------------------\n",
    "#IQR identifies outliers\n",
    "Q1 = df_2019_ln_k[['ln_y', 'ln_k']].quantile(0.25)\n",
    "Q3 = df_2019_ln_k[['ln_y', 'ln_k']].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "#Outliers are values of ln_y or ln_k that are 1.5 * IQR lower than the first quartile or (using |) 1.5 * IQR higher than the third quartile\n",
    "outliers = ((df_2019_ln_k[['ln_y', 'ln_k']] < (Q1 - 1.5 * IQR)) | (df_2019_ln_k[['ln_y', 'ln_k']] > (Q3 + 1.5 * IQR))).any(axis = 1) \n",
    "no_outliers_ln_k = df_2019_ln_k[~outliers] #Creates a new dataset that does not include outliers for ln(k)\n",
    "\n",
    "#Scatterplot--------------------------------\n",
    "#Produces scatterplot as before\n",
    "plt.scatter(no_outliers_ln_k['ln_y'], no_outliers_ln_k['ln_k'])\n",
    "plt.xlabel(\"ln(y) (natural logarithm of income per worker)\")\n",
    "plt.ylabel(\"ln(k) (natural logarithm of physical capital per worker)\")\n",
    "plt.title(\"Scatterplot for ln(k) against ln(y) (No outliers)\")\n",
    "\n",
    "#Regression line---------------------------\n",
    "reg_k_coefficients_no_outliers = np.polyfit(no_outliers_ln_k['ln_y'], no_outliers_ln_k['ln_k'], 1)\n",
    "x_vals_no_outliers = np.linspace(no_outliers_ln_k['ln_y'].min(), no_outliers_ln_k['ln_y'].max(), 100)\n",
    "y_vals_no_outliers = reg_k_coefficients_no_outliers[0] * x_vals_no_outliers + reg_k_coefficients_no_outliers[1]\n",
    "sign = \"+\" if reg_k_coefficients_no_outliers[1] >= 0 else \"-\"\n",
    "plt.plot(x_vals_no_outliers, y_vals_no_outliers, color = 'xkcd:blood',  #Colour changed to differentiate from diagram including outliers\n",
    "         label = f\"ln(k) = {reg_k_coefficients_no_outliers[0]:.2f}ln(y) {sign} {abs(reg_k_coefficients_no_outliers[1]):.2f}\")\n",
    "\n",
    "#Percentiles--------------------------------\n",
    "for percentile in percentiles:\n",
    "    ln_k_percentile_value = np.percentile(no_outliers_ln_k['ln_k'], percentile)\n",
    "\n",
    "    closest_country_idx = np.abs(no_outliers_ln_k['ln_k'] - ln_k_percentile_value).idxmin()\n",
    "    country_at_percentile = no_outliers_ln_k.loc[closest_country_idx, 'countrycode']\n",
    "\n",
    "    plt.text(no_outliers_ln_k.loc[no_outliers_ln_k['countrycode'] == country_at_percentile, 'ln_y'],\n",
    "             no_outliers_ln_k.loc[no_outliers_ln_k['countrycode'] == country_at_percentile, 'ln_k'],\n",
    "             f'{country_at_percentile}\\n{percentile}th', fontsize = 9, color = 'black', ha = 'right')\n",
    "    plt.annotate(\"\",\n",
    "                 xy = (no_outliers_ln_k.loc[no_outliers_ln_k['countrycode'] == country_at_percentile, 'ln_y'].values[0],\n",
    "                     no_outliers_ln_k.loc[no_outliers_ln_k['countrycode'] == country_at_percentile, 'ln_k'].values[0]),\n",
    "                 xytext = (no_outliers_ln_k.loc[no_outliers_ln_k['countrycode'] == country_at_percentile, 'ln_y'].values[0] + 0.2,\n",
    "                         no_outliers_ln_k.loc[no_outliers_ln_k['countrycode'] == country_at_percentile, 'ln_k'].values[0] + 0.2),\n",
    "                 arrowprops = dict(color = 'black', arrowstyle = '->'))\n",
    "\n",
    "#Final-------------------------------------\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. (ii): ln(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outliers------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#Scatterplot------------------------------\n",
    "#Calculates ln(h), and drops NaN values\n",
    "df_2019_ln_h = df_2019.dropna(subset = ['rgdpna', 'hc'])\n",
    "df_2019_ln_h['ln_h'] = np.log(df_2019_ln_h['hc'])\n",
    "\n",
    "plt.scatter(df_2019_ln_h['ln_y'], df_2019_ln_h['ln_h'])\n",
    "plt.xlabel(\"ln(y) (natural logarithm of income per worker)\")\n",
    "plt.ylabel(\"ln(h) (natural logarithm of human capital per worker)\")\n",
    "plt.title(\"Scatterplot for ln(h) against ln(y)\")\n",
    "\n",
    "#Regression line-------------------------\n",
    "reg_h_coefficients = np.polyfit(df_2019_ln_h['ln_y'], df_2019_ln_h['ln_h'], 1)\n",
    "x_vals = np.linspace(df_2019_ln_h['ln_y'].min(), df_2019_ln_h['ln_y'].max(), 100)\n",
    "y_vals = reg_h_coefficients[0] * x_vals + reg_h_coefficients[1]\n",
    "sign = \"+\" if reg_h_coefficients[1] >= 0 else \"-\"\n",
    "plt.plot(x_vals, y_vals, color = 'red', label = f\"ln(y) = {reg_h_coefficients[0]:.2f}ln(h) {sign} {abs(reg_h_coefficients[1]):.2f}\")\n",
    "\n",
    "#Percentiles-----------------------------\n",
    "for percentile in percentiles:\n",
    "    ln_h_percentile_value = np.percentile(df_2019_ln_h['ln_h'], percentile) \n",
    "    \n",
    "    closest_country_idx = np.abs(df_2019_ln_h['ln_h'] - ln_h_percentile_value).idxmin() \n",
    "    country_at_percentile = df_2019_ln_h.loc[closest_country_idx, 'countrycode']\n",
    "    \n",
    "    plt.text(df_2019_ln_h.loc[df_2019_ln_h['countrycode'] == country_at_percentile, 'ln_y'],\n",
    "             df_2019_ln_h.loc[df_2019_ln_h['countrycode'] == country_at_percentile, 'ln_h'],\n",
    "             f'{country_at_percentile}\\n{percentile}th', fontsize = 9, ha = 'right') \n",
    "    plt.annotate(\"\", \n",
    "                 xy = (df_2019_ln_h.loc[df_2019_ln_h['countrycode'] == country_at_percentile, 'ln_y'].values[0],\n",
    "                     df_2019_ln_h.loc[df_2019_ln_h['countrycode'] == country_at_percentile, 'ln_h'].values[0]),\n",
    "                 xytext = (df_2019_ln_h.loc[df_2019_ln_h['countrycode'] == country_at_percentile, 'ln_y'].values[0] + 0.3, \n",
    "                         df_2019_ln_h.loc[df_2019_ln_h['countrycode'] == country_at_percentile, 'ln_h'].values[0] + 0.05),\n",
    "                 arrowprops = dict(color = 'black', arrowstyle = '->'))\n",
    "\n",
    "#Final-----------------------------------\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#No Outliers-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#Identifying outliers--------------------\n",
    "Q1 = df_2019_ln_h[['ln_y', 'ln_h']].quantile(0.25)\n",
    "Q3 = df_2019_ln_h[['ln_y', 'ln_h']].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = ((df_2019_ln_h[['ln_y', 'ln_h']] < (Q1 - 1.5 * IQR)) | (df_2019_ln_h[['ln_y', 'ln_h']] > (Q3 + 1.5 * IQR))).any(axis = 1) \n",
    "no_outliers_ln_h = df_2019_ln_h[~outliers]\n",
    "\n",
    "#Scatterplot----------------------------\n",
    "plt.scatter(no_outliers_ln_h['ln_y'], no_outliers_ln_h['ln_h'])\n",
    "plt.xlabel(\"ln(y) (natural logarithm of income per worker)\")\n",
    "plt.ylabel(\"ln(h) (natural logarithm of human capital per worker)\")\n",
    "plt.title(\"Scatterplot for ln(h) against ln(y) (No outliers)\")\n",
    "\n",
    "#Regression line------------------------\n",
    "reg_h_coefficients_no_outliers = np.polyfit(no_outliers_ln_h['ln_y'], no_outliers_ln_h['ln_h'], 1)\n",
    "x_vals_no_outliers = np.linspace(no_outliers_ln_h['ln_y'].min(), no_outliers_ln_h['ln_y'].max(), 100)\n",
    "y_vals_no_outliers = reg_h_coefficients_no_outliers[0] * x_vals_no_outliers + reg_h_coefficients_no_outliers[1]\n",
    "sign = \"+\" if reg_h_coefficients_no_outliers[1] >= 0 else \"-\"\n",
    "plt.plot(x_vals_no_outliers, y_vals_no_outliers, color = 'xkcd:blood', label = f\"ln(y) = {reg_h_coefficients_no_outliers[0]:.2f}ln(h) {sign} {abs(reg_h_coefficients_no_outliers[1]):.2f}\")\n",
    "\n",
    "#Percentiles----------------------------\n",
    "for percentile in percentiles:\n",
    "    ln_h_percentile_value = np.percentile(no_outliers_ln_h['ln_h'], percentile) \n",
    "    \n",
    "    closest_country_idx = np.abs(no_outliers_ln_h['ln_h'] - ln_h_percentile_value).idxmin() \n",
    "    country_at_percentile = no_outliers_ln_h.loc[closest_country_idx, 'countrycode']\n",
    "    \n",
    "    plt.text(no_outliers_ln_h.loc[no_outliers_ln_h['countrycode'] == country_at_percentile, 'ln_y'],\n",
    "             no_outliers_ln_h.loc[no_outliers_ln_h['countrycode'] == country_at_percentile, 'ln_h'],\n",
    "             f'{country_at_percentile}\\n{percentile}th', fontsize = 9, ha = 'right') \n",
    "    plt.annotate(\"\", \n",
    "                 xy = (no_outliers_ln_h.loc[no_outliers_ln_h['countrycode'] == country_at_percentile, 'ln_y'].values[0],\n",
    "                     no_outliers_ln_h.loc[no_outliers_ln_h['countrycode'] == country_at_percentile, 'ln_h'].values[0]),\n",
    "                 xytext = (no_outliers_ln_h.loc[no_outliers_ln_h['countrycode'] == country_at_percentile, 'ln_y'].values[0] + 0.3, \n",
    "                         no_outliers_ln_h.loc[no_outliers_ln_h['countrycode'] == country_at_percentile, 'ln_h'].values[0] + 0.05),\n",
    "                         arrowprops = dict(color = 'black', arrowstyle = '->'))\n",
    "\n",
    "#Final----------------------------------\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. (iii): avh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outliers------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#Scatterplot------------------------------\n",
    "df_2019_avh = df_2019.dropna(subset = ['rgdpna', 'avh'])\n",
    "\n",
    "plt.scatter(df_2019_avh['ln_y'], df_2019_avh['avh'])\n",
    "plt.xlabel(\"ln(y) (natural logarithm of income per worker)\")\n",
    "plt.ylabel(\"avh (average annual hours worked)\")\n",
    "plt.title(\"Scatterplot for avh against ln(y)\")\n",
    "\n",
    "#Regression line--------------------------\n",
    "reg_avh_coefficients = np.polyfit(df_2019_avh['ln_y'], df_2019_avh['avh'], 1)\n",
    "x_vals = np.linspace(df_2019_avh['ln_y'].min(), df_2019_avh['ln_y'].max(), 100)\n",
    "y_vals = reg_avh_coefficients[0] * x_vals + reg_avh_coefficients[1]\n",
    "sign = \"+\" if reg_avh_coefficients[1] >= 0 else \"-\"\n",
    "plt.plot(x_vals, y_vals, color = 'red', label = f\"avh = {reg_avh_coefficients[0]:.2f}ln(y) {sign} {abs(reg_avh_coefficients[1]):.2f}\") \n",
    "\n",
    "#Percentiles-----------------------------\n",
    "for percentile in percentiles:\n",
    "    avh_percentile_value = np.percentile(df_2019_avh['avh'], percentile) \n",
    "    \n",
    "    closest_country_idx = np.abs(df_2019_avh['avh'] - avh_percentile_value).idxmin() \n",
    "    country_at_percentile = df_2019_avh.loc[closest_country_idx, 'countrycode']\n",
    "    \n",
    "    plt.text(df_2019_avh.loc[df_2019_avh['countrycode'] == country_at_percentile, 'ln_y'],\n",
    "             df_2019_avh.loc[df_2019_avh['countrycode'] == country_at_percentile, 'avh'],\n",
    "             f'{country_at_percentile}\\n{percentile}th', fontsize = 9, ha = 'right') \n",
    "    plt.annotate(\"\", \n",
    "                 xy = (df_2019_avh.loc[df_2019_avh['countrycode'] == country_at_percentile, 'ln_y'].values[0],\n",
    "                     df_2019_avh.loc[df_2019_avh['countrycode'] == country_at_percentile, 'avh'].values[0]),\n",
    "                 xytext = (df_2019_avh.loc[df_2019_avh['countrycode'] == country_at_percentile, 'ln_y'].values[0] + 0.2, \n",
    "                         df_2019_avh.loc[df_2019_avh['countrycode'] == country_at_percentile, 'avh'].values[0] + 50),\n",
    "                 arrowprops = dict(color = 'black', arrowstyle = '->'))\n",
    "\n",
    "#Final-----------------------------------\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#No Outliers-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#Identifying outliers--------------------\n",
    "Q1 = df_2019_avh[['ln_y', 'avh']].quantile(0.25)\n",
    "Q3 = df_2019_avh[['ln_y', 'avh']].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = ((df_2019_avh[['ln_y', 'avh']] < (Q1 - 1.5 * IQR)) | (df_2019_avh[['ln_y', 'avh']] > (Q3 + 1.5 * IQR))).any(axis = 1) \n",
    "no_outliers_avh = df_2019_avh[~outliers]\n",
    "\n",
    "#Scatterplot----------------------------\n",
    "plt.scatter(no_outliers_avh['ln_y'], no_outliers_avh['avh'])\n",
    "plt.xlabel(\"ln(y) (natural logarithm of income per worker)\")\n",
    "plt.ylabel(\"avh (average annual hours worked)\")\n",
    "plt.title(\"Scatterplot for avh against ln(y) (No outliers)\")\n",
    "\n",
    "#Regression line-----------------------\n",
    "reg_avh_coefficients_no_outliers = np.polyfit(no_outliers_avh['ln_y'], no_outliers_avh['avh'], 1)\n",
    "x_vals_no_outliers = np.linspace(no_outliers_avh['ln_y'].min(), no_outliers_avh['ln_y'].max(), 100)\n",
    "y_vals_no_outliers = reg_avh_coefficients_no_outliers[0] * x_vals_no_outliers + reg_avh_coefficients_no_outliers[1]\n",
    "sign = \"+\" if reg_avh_coefficients_no_outliers[1] >= 0 else \"-\"\n",
    "plt.plot(x_vals_no_outliers, y_vals_no_outliers, color = 'xkcd:blood', label = f\"avh = {reg_avh_coefficients_no_outliers[0]:.2f}ln(y) {sign} {abs(reg_avh_coefficients_no_outliers[1]):.2f}\")\n",
    "\n",
    "#Percentiles-----------------------------\n",
    "for percentile in percentiles:\n",
    "    avh_percentile_value = np.percentile(no_outliers_avh['avh'], percentile) \n",
    "    \n",
    "    closest_country_idx = np.abs(no_outliers_avh['avh'] - avh_percentile_value).idxmin() \n",
    "    country_at_percentile = no_outliers_avh.loc[closest_country_idx, 'countrycode']\n",
    "    \n",
    "    plt.text(no_outliers_avh.loc[no_outliers_avh['countrycode'] == country_at_percentile, 'ln_y'],\n",
    "             no_outliers_avh.loc[no_outliers_avh['countrycode'] == country_at_percentile, 'avh'],\n",
    "             f'{country_at_percentile}\\n{percentile}th', fontsize = 9, ha = 'right') \n",
    "    plt.annotate(\"\", \n",
    "                 xy = (no_outliers_avh.loc[no_outliers_avh['countrycode'] == country_at_percentile, 'ln_y'].values[0],\n",
    "                     no_outliers_avh.loc[no_outliers_avh['countrycode'] == country_at_percentile, 'avh'].values[0]),\n",
    "                 xytext = (no_outliers_avh.loc[no_outliers_avh['countrycode'] == country_at_percentile, 'ln_y'].values[0] + 0.2, \n",
    "                         no_outliers_avh.loc[no_outliers_avh['countrycode'] == country_at_percentile, 'avh'].values[0] + 50),\n",
    "                 arrowprops =dict(color = 'black', arrowstyle = '->'))\n",
    "\n",
    "#Final--------------------------------\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. (iv): (1 ‚àí ùõº)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outliers------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#Scatterplot------------------------------\n",
    "df_2019_labsh = df_2019.dropna(subset = ['rgdpna', 'labsh'])\n",
    "\n",
    "plt.scatter(df_2019_labsh['ln_y'], df_2019_labsh['labsh'])\n",
    "plt.xlabel(\"ln(y) (natural logarithm of income per worker)\")\n",
    "plt.ylabel(\"$(1 ‚àí ùõº)$ (share of labour compensation in GDP)\") #Using $ so that this text is read as LaTeX\n",
    "plt.title(\"Scatterplot for $(1 ‚àí ùõº)$ against ln(y)\")\n",
    "\n",
    "#Regression line-------------------------\n",
    "reg_labsh_coefficients = np.polyfit(df_2019_labsh['ln_y'], df_2019_labsh['labsh'], 1)\n",
    "x_vals = np.linspace(df_2019_labsh['ln_y'].min(), df_2019_labsh['ln_y'].max(), 100)\n",
    "y_vals = reg_labsh_coefficients[0] * x_vals + reg_labsh_coefficients[1]\n",
    "sign = \"+\" if reg_labsh_coefficients[1] >= 0 else \"-\"\n",
    "plt.plot(x_vals, y_vals, color = 'red', label = f\"$(1 ‚àí ùõº)$ = {reg_labsh_coefficients[0]:.4f}ln(y) {sign} {abs(reg_labsh_coefficients[1]):.2f}\") #Changed to 4 d.p. as previously showed 0.00\n",
    "\n",
    "#Percentiles-----------------------------\n",
    "for percentile in percentiles:\n",
    "    labsh_percentile_value = np.percentile(df_2019_labsh['labsh'], percentile) \n",
    "    \n",
    "    closest_country_idx = np.abs(df_2019_labsh['labsh'] - labsh_percentile_value).idxmin() \n",
    "    country_at_percentile = df_2019_labsh.loc[closest_country_idx, 'countrycode']\n",
    "    \n",
    "    plt.text(df_2019_labsh.loc[df_2019_labsh['countrycode'] == country_at_percentile, 'ln_y'],\n",
    "             df_2019_labsh.loc[df_2019_labsh['countrycode'] == country_at_percentile, 'labsh'],\n",
    "             f'{country_at_percentile}\\n{percentile}th', fontsize = 9, ha = 'right') \n",
    "    plt.annotate(\"\", \n",
    "                 xy = (df_2019_labsh.loc[df_2019_labsh['countrycode'] == country_at_percentile, 'ln_y'].values[0],\n",
    "                     df_2019_labsh.loc[df_2019_labsh['countrycode'] == country_at_percentile, 'labsh'].values[0]),\n",
    "                 xytext = (df_2019_labsh.loc[df_2019_labsh['countrycode'] == country_at_percentile, 'ln_y'].values[0] + 0.2, \n",
    "                         df_2019_labsh.loc[df_2019_labsh['countrycode'] == country_at_percentile, 'labsh'].values[0] + 0.025),\n",
    "                 arrowprops = dict(color = 'black', arrowstyle = '->'))\n",
    "\n",
    "#Final-----------------------------------\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#No Outliers-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#Identifying outliers--------------------\n",
    "Q1 = df_2019_labsh[['ln_y', 'labsh']].quantile(0.25)\n",
    "Q3 = df_2019_labsh[['ln_y', 'labsh']].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = ((df_2019_labsh[['ln_y', 'labsh']] < (Q1 - 1.5 * IQR)) | (df_2019_labsh[['ln_y', 'labsh']] > (Q3 + 1.5 * IQR))).any(axis = 1) \n",
    "no_outliers_labsh = df_2019_labsh[~outliers]\n",
    "\n",
    "#Scatterplot-----------------------------\n",
    "plt.scatter(no_outliers_labsh['ln_y'], no_outliers_labsh['labsh'])\n",
    "plt.xlabel(\"ln(y) (natural logarithm of income per worker)\")\n",
    "plt.ylabel(\"$(1 ‚àí ùõº)$ (share of labour compensation in GDP)\")\n",
    "plt.title(\"Scatterplot for $(1 ‚àí ùõº)$ against ln(y) (No outliers)\")\n",
    "\n",
    "#Regression line------------------------\n",
    "reg_labsh_coefficients_no_outliers = np.polyfit(no_outliers_labsh['ln_y'], no_outliers_labsh['labsh'], 1)\n",
    "x_vals_no_outliers = np.linspace(no_outliers_labsh['ln_y'].min(), no_outliers_labsh['ln_y'].max(), 100)\n",
    "y_vals_no_outliers = reg_labsh_coefficients_no_outliers[0] * x_vals_no_outliers + reg_labsh_coefficients_no_outliers[1]\n",
    "sign = \"+\" if reg_labsh_coefficients_no_outliers[1] >= 0 else \"-\"\n",
    "plt.plot(x_vals_no_outliers, y_vals_no_outliers, color = 'xkcd:blood', label = f\"$(1 ‚àí ùõº)$ = {reg_labsh_coefficients_no_outliers[0]:.4f}ln(y) {sign} {abs(reg_labsh_coefficients_no_outliers[1]):.2f}\")\n",
    "\n",
    "#Percentiles-----------------------------\n",
    "for percentile in percentiles:\n",
    "    labsh_percentile_value = np.percentile(no_outliers_labsh['labsh'], percentile) \n",
    "    \n",
    "    closest_country_idx = np.abs(no_outliers_labsh['labsh'] - labsh_percentile_value).idxmin() \n",
    "    country_at_percentile = no_outliers_labsh.loc[closest_country_idx, 'countrycode']\n",
    "    \n",
    "    plt.text(no_outliers_labsh.loc[no_outliers_labsh['countrycode'] == country_at_percentile, 'ln_y'],\n",
    "            no_outliers_labsh.loc[no_outliers_labsh['countrycode'] == country_at_percentile, 'labsh'],\n",
    "             f'{country_at_percentile}\\n{percentile}th', fontsize = 9, ha = 'right') \n",
    "    plt.annotate(\"\", \n",
    "                 xy = (no_outliers_labsh.loc[no_outliers_labsh['countrycode'] == country_at_percentile, 'ln_y'].values[0],\n",
    "                     no_outliers_labsh.loc[no_outliers_labsh['countrycode'] == country_at_percentile, 'labsh'].values[0]),\n",
    "                 xytext = (no_outliers_labsh.loc[no_outliers_labsh['countrycode'] == country_at_percentile, 'ln_y'].values[0] + 0.2, \n",
    "                         no_outliers_labsh.loc[no_outliers_labsh['countrycode'] == country_at_percentile, 'labsh'].values[0] + 0.025),\n",
    "                 arrowprops = dict(color = 'black', arrowstyle = '->'))\n",
    "\n",
    "#Final----------------------------------\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. (v) ln(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outliers------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#Scatterplot------------------------------\n",
    "df_2019_rtfpna = df_2019.dropna(subset = ['rgdpna', 'rtfpna'])\n",
    "df_2019_rtfpna['rtfpna (log)'] = np.log(df_2019_rtfpna['rtfpna'])\n",
    "\n",
    "plt.scatter(df_2019_rtfpna['ln_y'], df_2019_rtfpna['rtfpna (log)'])\n",
    "plt.xlabel(\"ln(y) (natural logarithm of income per worker)\")\n",
    "plt.ylabel(\"ln(A) (natural logarithm of total factor productivity)\") \n",
    "plt.title(\"Scatterplot for ln(A) against ln(y)\")\n",
    "\n",
    "#Regression line-------------------------\n",
    "reg_rtfpna_coefficients = np.polyfit(df_2019_rtfpna['ln_y'], df_2019_rtfpna['rtfpna (log)'], 1)\n",
    "x_vals = np.linspace(df_2019_rtfpna['ln_y'].min(), df_2019_rtfpna['ln_y'].max(), 100)\n",
    "y_vals = reg_rtfpna_coefficients[0] * x_vals + reg_rtfpna_coefficients[1]\n",
    "sign = \"+\" if reg_rtfpna_coefficients[1] >= 0 else \"-\"\n",
    "plt.plot(x_vals, y_vals, color = 'red', label = f\"ln(A) = {reg_rtfpna_coefficients[0]:.2f}ln(y) {sign} {abs(reg_rtfpna_coefficients[1]):.2f}\")\n",
    "\n",
    "#Percentiles-----------------------------\n",
    "for percentile in percentiles:\n",
    "    rtfpna_percentile_value = np.percentile(df_2019_rtfpna['rtfpna (log)'], percentile) \n",
    "    \n",
    "    closest_country_idx = np.abs(df_2019_rtfpna['rtfpna (log)'] - rtfpna_percentile_value).idxmin() \n",
    "    country_at_percentile = df_2019_rtfpna.loc[closest_country_idx, 'countrycode']\n",
    "\n",
    "    plt.text(df_2019_rtfpna.loc[df_2019_rtfpna['countrycode'] == country_at_percentile, 'ln_y'],\n",
    "             df_2019_rtfpna.loc[df_2019_rtfpna['countrycode'] == country_at_percentile, 'rtfpna (log)'],\n",
    "             f'{country_at_percentile}\\n{percentile}th', fontsize = 9, ha = 'right') \n",
    "    plt.annotate(\"\", \n",
    "                 xy = (df_2019_rtfpna.loc[df_2019_rtfpna['countrycode'] == country_at_percentile, 'ln_y'].values[0],\n",
    "                     df_2019_rtfpna.loc[df_2019_rtfpna['countrycode'] == country_at_percentile, 'rtfpna (log)'].values[0]),\n",
    "                 xytext = (df_2019_rtfpna.loc[df_2019_rtfpna['countrycode'] == country_at_percentile, 'ln_y'].values[0] + 0.2, \n",
    "                         df_2019_rtfpna.loc[df_2019_rtfpna['countrycode'] == country_at_percentile, 'rtfpna (log)'].values[0] + 0.025),\n",
    "                 arrowprops = dict(color = 'black', arrowstyle = '->'))\n",
    "\n",
    "#Final-----------------------------------\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#No Outliers-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#Identifying outliers--------------------\n",
    "Q1 = df_2019_rtfpna[['ln_y', 'rtfpna (log)']].quantile(0.25)\n",
    "Q3 = df_2019_rtfpna[['ln_y', 'rtfpna (log)']].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = ((df_2019_rtfpna[['ln_y', 'rtfpna (log)']] < (Q1 - 1.5 * IQR)) | (df_2019_rtfpna[['ln_y', 'rtfpna (log)']] > (Q3 + 1.5 * IQR))).any(axis = 1) \n",
    "no_outliers_rtfpna = df_2019_rtfpna[~outliers]\n",
    "\n",
    "#Scatterplot-----------------------------\n",
    "plt.scatter(no_outliers_rtfpna['ln_y'], no_outliers_rtfpna['rtfpna (log)'])\n",
    "plt.xlabel(\"ln(y) (natural logarithm of income per worker)\")\n",
    "plt.ylabel(\"ln(A) (natural logarithm of total factor productivity)\")\n",
    "plt.title(\"Scatterplot for ln(A) against ln(y) (No outliers)\")\n",
    "\n",
    "#Regression line------------------------\n",
    "reg_rtfpna_coefficients_no_outliers = np.polyfit(no_outliers_rtfpna['ln_y'], no_outliers_rtfpna['rtfpna (log)'], 1)\n",
    "x_vals_no_outliers = np.linspace(no_outliers_rtfpna['ln_y'].min(), no_outliers_rtfpna['ln_y'].max(), 100)\n",
    "y_vals_no_outliers = reg_rtfpna_coefficients_no_outliers[0] * x_vals_no_outliers + reg_rtfpna_coefficients_no_outliers[1]\n",
    "sign = \"+\" if reg_rtfpna_coefficients_no_outliers[1] >= 0 else \"-\"\n",
    "plt.plot(x_vals_no_outliers, y_vals_no_outliers, color = 'xkcd:blood', label = f\"ln(A) = {reg_rtfpna_coefficients_no_outliers[0]:.4f}ln(y) {sign} {abs(reg_rtfpna_coefficients_no_outliers[1]):.2f}\")\n",
    "\n",
    "#Percentiles-----------------------------\n",
    "for percentile in percentiles:\n",
    "    rtfpna_percentile_value = np.percentile(no_outliers_rtfpna['rtfpna (log)'], percentile) \n",
    "    \n",
    "    closest_country_idx = np.abs(no_outliers_rtfpna['rtfpna (log)'] - rtfpna_percentile_value).idxmin() \n",
    "    country_at_percentile = no_outliers_rtfpna.loc[closest_country_idx, 'countrycode']\n",
    "\n",
    "    plt.text(no_outliers_rtfpna.loc[no_outliers_rtfpna['countrycode'] == country_at_percentile, 'ln_y'],\n",
    "            no_outliers_rtfpna.loc[no_outliers_rtfpna['countrycode'] == country_at_percentile, 'rtfpna (log)'],\n",
    "             f'{country_at_percentile}\\n{percentile}th', fontsize = 9, ha = 'right') \n",
    "    plt.annotate(\"\", \n",
    "                 xy = (no_outliers_rtfpna.loc[no_outliers_rtfpna['countrycode'] == country_at_percentile, 'ln_y'].values[0],\n",
    "                     no_outliers_rtfpna.loc[no_outliers_rtfpna['countrycode'] == country_at_percentile, 'rtfpna (log)'].values[0]),\n",
    "                 xytext = (no_outliers_rtfpna.loc[no_outliers_rtfpna['countrycode'] == country_at_percentile, 'ln_y'].values[0] + 0.2, \n",
    "                         no_outliers_rtfpna.loc[no_outliers_rtfpna['countrycode'] == country_at_percentile, 'rtfpna (log)'].values[0] + 0.005),\n",
    "                 arrowprops = dict(color = 'black', arrowstyle = '->'))\n",
    "\n",
    "#Final----------------------------------\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at all the scatter plots visually, it seems that (1 ‚àí ùõº) or ln(A) have the most variance across the development spectrum. At the very least, it seems that ln(k) has the least variance. It is not simple to compare variances; below I have calculated the coefficient of variation for each variable, but given ln(A)'s very low mean, this has led to a very high coefficient of variation. It's interesting to note that ln(h)'s coefficient of variance is higher than (1 ‚àí ùõº)'s, though again, this is likely due to the small size of ln(h)'s mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_ln_k = (np.std(df_2019_ln_k['ln_k']) / np.mean(df_2019_ln_k['ln_k'])) * 100\n",
    "print(f\"The coefficient of variation for ln(k) is {var_ln_k:.2f}%\")\n",
    "\n",
    "var_ln_h = (np.std(df_2019_ln_h['ln_h']) / np.mean(df_2019_ln_h['ln_h'])) * 100\n",
    "print(f\"The coefficient of variation for ln(h) is {var_ln_h:.2f}%\")\n",
    "\n",
    "var_avh = (np.std(df_2019_avh['avh']) / np.mean(df_2019_avh['avh'])) * 100\n",
    "print(f\"The coefficient of variation for avh is {var_avh:.2f}%\")\n",
    "\n",
    "var_labsh = (np.std(df_2019_labsh['labsh']) / np.mean(df_2019_labsh['labsh'])) * 100\n",
    "print(f\"The coefficient of variation for labsh is {var_labsh:.2f}%\")\n",
    "\n",
    "var_ln_A = (np.std(df_2019_rtfpna['rtfpna (log)']) / abs(np.mean(df_2019_rtfpna['rtfpna (log)']))) * 100\n",
    "print(f\"The coefficient of variation for ln(A) is {var_ln_A:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Compute these measures of success in your PWT sample. As in Caselli, report these results in a table including the resulting measure of success, the number of observations, and the elements used to compute your measures of success (i.e. ùë£ùëéùëü[ln(ùë¶ùëò‚Ñé)], ùë£ùëéùëü[ln(ùë¶)]).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop NaN values in variables of interest\n",
    "df_2019_cleaned = df_2019.dropna(subset = ['rgdpna', 'rnna', 'emp', 'avh', 'hc', 'rtfpna'])\n",
    "\n",
    "#Define variables\n",
    "alpha = 1 / 3\n",
    "k = df_2019_cleaned['rnna'] / df_2019_cleaned['emp']\n",
    "avh = df_2019_cleaned['avh']\n",
    "h = df_2019_cleaned['hc']\n",
    "y = df_2019_cleaned['rgdpna'] / df_2019_cleaned['emp']\n",
    "y_kh = k**alpha * (avh * h)**(1 - alpha)\n",
    "A = df_2019_cleaned['rtfpna']\n",
    "A_tilde = y / y_kh\n",
    "\n",
    "#Calculate measures of success\n",
    "success_1 = np.var(np.log(y_kh)) / np.var(np.log(y))\n",
    "success_2 = np.var(np.log(A_tilde)) / np.var(np.log(y))\n",
    "success_3 = np.var(np.log(A)) / np.var(np.log(y))\n",
    "\n",
    "#Calculating different percentiles (Had to add [~np.isnan(y)] for success_4 and success_5 due to NaN output)\n",
    "y_kh_90_10 = np.percentile(y_kh, 90) / np.percentile(y_kh, 10)\n",
    "y_90_10 = np.percentile(y[~np.isnan(y)], 90) / np.percentile(y[~np.isnan(y)], 10)\n",
    "y_kh_75_25 = np.percentile(y_kh, 75) / np.percentile(y_kh, 25)\n",
    "y_75_25 = np.percentile(y[~np.isnan(y)], 75) / np.percentile(y[~np.isnan(y)], 25)\n",
    "\n",
    "success_4 = y_kh_90_10 / y_90_10 \n",
    "success_5 = y_kh_75_25 / y_75_25\n",
    "\n",
    "#Create dictionary including measures of success, observations, and intuition\n",
    "success_measures = {\"Measure of success\": ['var(ln(y))', 'var(ln(y_kh))', 'success_1', 'success_2', 'success_3', 'success_4', 'success_5'],\n",
    "                    \"Intuition\": [\"Variance of natural logarithm of income per worker\", \"Variance of natural logarithm of the modified production's income per worker\",\n",
    "                                  \"Share of variance in income per worker explained by factors of production\", \"Share of variance in income per worker explained by efficiency\",\n",
    "                                  \"Share of variance in income per worker explained by TFP\", \"90th and 10th percentiles in modified income compared to standard income\",\n",
    "                                  \"75th and 25th percentiles in modified income compared to standard income\"],\n",
    "                    \"Observations\": [len(y), len(y_kh), len(y_kh), len(A_tilde), len(A), len(y_kh), len(y_kh)],\n",
    "                    \"Value\": [np.var(np.log(y)), np.var(np.log(y_kh)), success_1, success_2, success_3, success_4, success_5]}\n",
    "\n",
    "success_table = pd.DataFrame(success_measures)\n",
    "print(\"Table of findings:\")\n",
    "print(success_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Keeping ùõº = 1/3 fixed, we can decompose ùë£ùëéùëü[ln(ùë¶_ùëò‚Ñé)] as ùë£ùëéùëü(ùëôùëõ(ùë¶_ùëò‚Ñé)) = ùõº^2 ùë£ùëéùëü(ùëôùëõ(ùëò)) + (1‚àí ùõº)^2 ùë£ùëéùëü(ùëôùëõ(ùëéùë£‚Ñé ‚àô ‚Ñé)) + 2ùõº(1‚àí ùõº)ùëêùëúùë£(ùëôùëõ(ùëò), ùëôùëõ(ùëéùë£‚Ñé ‚àô ‚Ñé)). Compute what share of the ùë£ùëéùëü(ùëôùëõ(ùë¶_ùëò‚Ñé)) that is explained by ùë£ùëéùëü(ùëôùëõ(ùëò)), ùë£ùëéùëü(ùëôùëõ(ùëéùë£‚Ñé ‚àô ‚Ñé)), and ùëêùëúùë£(ùëôùëõ(ùëò), ùëôùëõ(ùëéùë£‚Ñé ‚àô ‚Ñé)) respectively. Report your results in a table like the one provided in question 5. Perform a sensitivity analysis with respect to the value of ùõº by redoing this decomposition for values of ùõº equal to the minimum, maximum, and mean in your sample.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define variables and variances\n",
    "alpha = 1 / 3\n",
    "y_kh = k**alpha * (avh * h)**(1 - alpha)\n",
    "var_ln_y_kh = np.var(np.log(y_kh))\n",
    "var_ln_k = np.var(np.log(k))\n",
    "var_ln_avh_h = np.var(np.log(avh * h))\n",
    "\n",
    "#Calculates covariance between ln(k) and ln(avh * h)\n",
    "cov_ln_k_avh_h = np.cov(np.log(k), np.log(avh * h))[0, 1] #Element at [0, 1] in the variance-covariance matrix - the covariance between ln(k) and ln(avh * h) \n",
    "\n",
    "#Calculates share of var(ln(y_kh)) explained by var(ln(k))\n",
    "explained_ln_k = (alpha**2 * var_ln_k)\n",
    "share_ln_k = (explained_ln_k / var_ln_y_kh) * 100\n",
    "\n",
    "#Calculates share of var(ln(y_kh)) explained by var(ln(avh * h))\n",
    "explained_ln_avh_h = ((1 - alpha)**2 * var_ln_avh_h)\n",
    "share_ln_avh_h = (explained_ln_avh_h / var_ln_y_kh) * 100\n",
    "\n",
    "#Calculates share of var(ln(y_kh)) explained by cov(ln(k), ln(avh * h))\n",
    "explained_ln_k_avh_h = ((2 * alpha) * (1 - alpha) * cov_ln_k_avh_h)\n",
    "share_ln_k_avh_h = (explained_ln_k_avh_h / var_ln_y_kh) * 100\n",
    "\n",
    "shares = {\"Composite\": ['var(ln(k))', 'var(ln(avh * h))', 'cov(ln(k), ln(avh * h))'],\n",
    "                    \"Share of var(ln(y_kh))\": [share_ln_k, share_ln_avh_h, share_ln_k_avh_h]}\n",
    "\n",
    "shares_table = pd.DataFrame(shares)\n",
    "print(\"Table of findings:\")\n",
    "print(shares_table)\n",
    "\n",
    "#Minimum-------------------------------------------------------------------\n",
    "#Alpha is the share of GDP not comprised of labour compensation. In the dataset, the share of GDP comprised of labour compensation is labsh. Thus, alpha is 1 - labsh.\n",
    "alpha_min = np.min(1 - df_2019_cleaned['labsh'])\n",
    "y_kh_min = k**alpha_min * (avh * h)**(1 - alpha_min)\n",
    "var_ln_y_kh_min = np.var(np.log(y_kh_min))\n",
    "\n",
    "explained_ln_k_min = (alpha_min**2 * var_ln_k)\n",
    "share_ln_k_min = (explained_ln_k_min / var_ln_y_kh_min) * 100\n",
    "\n",
    "explained_ln_avh_h_min = ((1 - alpha_min)**2 * var_ln_avh_h)\n",
    "share_ln_avh_h_min = (explained_ln_avh_h_min / var_ln_y_kh_min) * 100\n",
    "\n",
    "explained_ln_k_avh_h_min = ((2 * alpha_min) * (1 - alpha_min) * cov_ln_k_avh_h)\n",
    "share_ln_k_avh_h_min = (explained_ln_k_avh_h_min / var_ln_y_kh_min) * 100\n",
    "\n",
    "shares_min = {\"Composite\": ['var(ln(k))', 'var(ln(avh * h))', 'cov(ln(k), ln(avh * h))'],\n",
    "                    \"Share of var(ln(y_kh))\": [share_ln_k_min, share_ln_avh_h_min, share_ln_k_avh_h_min]}\n",
    "\n",
    "shares_table_min = pd.DataFrame(shares_min)\n",
    "print()\n",
    "print(\"Table of findings (alpha = min):\")\n",
    "print(shares_table_min)\n",
    "\n",
    "#Maximum-------------------------------------------------------------------\n",
    "alpha_max = np.max(1 - df_2019_cleaned['labsh'])\n",
    "y_kh_max = k**alpha_max * (avh * h)**(1 - alpha_max)\n",
    "var_ln_y_kh_max = np.var(np.log(y_kh_max))\n",
    "\n",
    "explained_ln_k_max = (alpha_max**2 * var_ln_k)\n",
    "share_ln_k_max = (explained_ln_k_max / var_ln_y_kh_max) * 100\n",
    "\n",
    "explained_ln_avh_h_max = ((1 - alpha_max)**2 * var_ln_avh_h)\n",
    "share_ln_avh_h_max = (explained_ln_avh_h_max / var_ln_y_kh_max) * 100\n",
    "\n",
    "explained_ln_k_avh_h_max = ((2 * alpha_max) * (1 - alpha_max) * cov_ln_k_avh_h)\n",
    "share_ln_k_avh_h_max = (explained_ln_k_avh_h / var_ln_y_kh_max) * 100\n",
    "\n",
    "shares_max = {\"Composite\": ['var(ln(k))', 'var(ln(avh * h))', 'cov(ln(k), ln(avh * h))'],\n",
    "                    \"Share of var(ln(y_kh))\": [share_ln_k_max, share_ln_avh_h_max, share_ln_k_avh_h_max]}\n",
    "\n",
    "shares_table_max = pd.DataFrame(shares_max)\n",
    "print()\n",
    "print(\"Table of findings (alpha = max):\")\n",
    "print(shares_table_max)\n",
    "\n",
    "#Mean-------------------------------------------------------------------\n",
    "alpha_mean = np.mean(1 - df_2019_cleaned['labsh'])\n",
    "y_kh_mean = k**alpha_mean * (avh * h)**(1 - alpha_mean)\n",
    "var_ln_y_kh_mean = np.var(np.log(y_kh_mean))\n",
    "\n",
    "explained_ln_k_mean = (alpha_mean**2 * var_ln_k)\n",
    "share_ln_k_mean = (explained_ln_k_mean / var_ln_y_kh_mean) * 100\n",
    "\n",
    "explained_ln_avh_h_mean = ((1 - alpha_mean)**2 * var_ln_avh_h)\n",
    "share_ln_avh_h_mean = (explained_ln_avh_h_mean / var_ln_y_kh_mean) * 100\n",
    "\n",
    "explained_ln_k_avh_h_mean = ((2 * alpha_mean) * (1 - alpha_mean) * cov_ln_k_avh_h)\n",
    "share_ln_k_avh_h_mean = (explained_ln_k_avh_h / var_ln_y_kh_mean) * 100\n",
    "\n",
    "shares_mean = {\"Composite\": ['var(ln(k))', 'var(ln(avh * h))', 'cov(ln(k), ln(avh * h))'],\n",
    "                    \"Share of var(ln(y_kh))\": [share_ln_k_mean, share_ln_avh_h_mean, share_ln_k_avh_h_mean]}\n",
    "\n",
    "shares_table_mean = pd.DataFrame(shares_mean)\n",
    "print()\n",
    "print(\"Table of findings (alpha = mean):\")\n",
    "print(shares_table_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analysis shows that no matter the value of ùõº, var(ln(k)) comprises the majority of var(ln(y_kh)). The assumption that ùõº = 1 / 3 places our estimate at the lower end of the observed values for alpha. This value of alpha is clearly much lower than the mean of ùõº, which is 0.454. \n",
    "\n",
    "This also shows that the greater the value of ùõº, the greater the share of var(ln(y_kh)) explained by var(ln(k)). There is quite some variation in the composition of var(ln(y_kh)) dependent on the value of alpha, with the composition taken up by var(ln(k)) ranging from 73.25% when ùõº is at its minimum to 96.64% when ùõº is at its maximum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Based on your calculations: are differences in standards of living across countries mostly driven by factor accumulation or by the efficiency in which factors are used? Do your results depend on the measure of success used? Which factor of production is more important in explaining cross-country differences in income per-worker? How do your results compare to those in Caselli (2003)?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the output per worker function $y = \\widetilde{A} \\cdot k^{\\alpha} \\cdot (avh \\cdot h)^{1-\\alpha}$, we can split possible measures of success into two categories:\n",
    "- Success driven by factor accumulation\n",
    "- Success driven by factor efficiency\n",
    "\n",
    "Above, we have calculated $success_1$, which is a measure of what share of variance in income per country is explained by factors of production; essentially a measure of what portion of success is driven by factor accumulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_1_prc = success_1 * 100\n",
    "print(f\"{success_1_prc}% of the variation in income per country is explained by factors of production.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meanwhile, $success_2$ is a measure of what share of variance in income per country is explained by $\\widetilde{A}$, or, in other words, a measure of efficiency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_2_prc = success_2 * 100\n",
    "print(f\"{success_2_prc}% of the variation in income per country is explained by efficiency in factor use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that both factor accumulation and efficiency in using these factors are important, though efficiency more so. If we use different measures of success, we may get different results, as their values are quite variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_3_prc = success_3 * 100\n",
    "print(f\"{success_3_prc}% of the variation in income per country is explained by TFP.\")\n",
    "\n",
    "success_4_prc = success_4 * 100\n",
    "print(f\"{success_4_prc}% (90th percentile minus 10th percentile)\")\n",
    "\n",
    "success_5_prc = success_5 * 100\n",
    "print(f\"{success_5_prc}% (75th percentile minus 25th percentile).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use a similar method to before to see which factor of production is most important in explaining cross-country differences (assuming that ùõº = 1 / 3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1 / 3\n",
    "\n",
    "#Define variation of composites\n",
    "var_y = np.var(y)\n",
    "var_A = np.var(A)\n",
    "var_k_alpha = np.var(k**alpha)\n",
    "var_avh_alpha = np.var(avh**(1 - alpha))\n",
    "var_h = np.var(h)\n",
    "\n",
    "explained_A = (var_A)\n",
    "share_A = (explained_A / var_y) * 100\n",
    "\n",
    "explained_k = (var_k_alpha)\n",
    "share_k = (explained_k / var_y) * 100\n",
    "\n",
    "explained_avh = (var_avh_alpha)\n",
    "share_avh = (explained_avh / var_y) * 100\n",
    "\n",
    "explained_h = (var_h**(1 - alpha))\n",
    "share_h = (explained_h / var_y) * 100\n",
    "\n",
    "shares_y = {\"Composite\": ['var(A)', 'var(k)', 'var(avh)', 'var(h)'],\n",
    "                    \"Share of var(y)\": [share_A, share_k, share_avh, share_h]}\n",
    "\n",
    "shares_table_y = pd.DataFrame(shares_y)\n",
    "print(\"Table of findings:\")\n",
    "print(shares_table_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that k is the most important factor, followed by average hours worked. My value of $success_1$ of 0.2211 is lower than Caselli's calculation of 0.39, while my calculation of 0.5526 for $success_4$ ($success_2$ in Caselli) is larger than the 0.34 that Caselli found. As Caselli, I find that efficiency is very important for income per worker, along with capital accumulation. Overall, though, efficiency is more important for growth in income per worker than capital accumulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Can you think of any missing factors explaining differences in income per worker that we have not accounted for in this exercise? Provide a quantitative assessment of these factors either adding additional variables to your analysis or by modelling these factors accordingly. (For example, in section 4.4 Caselli argues that cross-country differences in health and nutrition status also play a role in explaining human capital differences. He models these effects in a simple, reduced-form way, and provides data to quantitatively assess the role of health gaps in explaining income differences across countries).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe a pertinent missing factor is institutional quality; institutional quality is thought to be an important driver of economic growth, and Nawaz, Iqbal & Khan (2014) show evidence that institutional quality improvements lead to a decrease in rent-seeking activity, and thus income increases. In this context, we could use statistical capacity (statcap) as a proxy for institutional quality - though an imperfect measure, one could imagine that countries more capable of disseminating accurate and extensive statistical information likely have higher-quality institutions. To investigate this possibility, I create a scatterplot between statistical capacity and log of income in order to see if there's any correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop NaN values\n",
    "df_2019_statcap = df_2019.dropna(subset = ['statcap', 'ln_y'])\n",
    "\n",
    "plt.scatter(df_2019_statcap['ln_y'], df_2019_statcap['statcap'])\n",
    "plt.xlabel(\"ln(y) (natural logarithm of income per worker)\")\n",
    "plt.ylabel(\"statcap (statistical capacity indicator)\")\n",
    "plt.title(\"Scatterplot for statistical capacity against ln(y)\")\n",
    "\n",
    "#Regression line-------------------------\n",
    "reg_statcap_coefficients = np.polyfit(df_2019_statcap['ln_y'], df_2019_statcap['statcap'], 1)\n",
    "x_vals = np.linspace(df_2019_statcap['ln_y'].min(), df_2019_statcap['ln_y'].max(), 100)\n",
    "y_vals = reg_statcap_coefficients[0] * x_vals + reg_statcap_coefficients[1]\n",
    "sign = \"+\" if reg_statcap_coefficients[1] >= 0 else \"-\"\n",
    "plt.plot(x_vals, y_vals, color = 'red', label = f\"statcap = {reg_statcap_coefficients[0]:.2f}ln(y) {sign} {abs(reg_statcap_coefficients[1]):.2f}\") \n",
    "\n",
    "#Percentiles-----------------------------\n",
    "for percentile in percentiles:\n",
    "    statcap_percentile_value = np.percentile(df_2019_statcap['statcap'], percentile) \n",
    "    \n",
    "    closest_country_idx = np.abs(df_2019_statcap['statcap'] - statcap_percentile_value).idxmin() \n",
    "    country_at_percentile = df_2019_statcap.loc[closest_country_idx, 'countrycode']\n",
    "    \n",
    "    plt.text(df_2019_statcap.loc[df_2019_statcap['countrycode'] == country_at_percentile, 'ln_y'],\n",
    "             df_2019_statcap.loc[df_2019_statcap['countrycode'] == country_at_percentile, 'statcap'],\n",
    "             f'{country_at_percentile}\\n{percentile}th', fontsize = 9, ha = 'right') \n",
    "    plt.annotate(\"\", \n",
    "                 xy = (df_2019_statcap.loc[df_2019_statcap['countrycode'] == country_at_percentile, 'ln_y'].values[0],\n",
    "                     df_2019_statcap.loc[df_2019_statcap['countrycode'] == country_at_percentile, 'statcap'].values[0]),\n",
    "                 xytext = (df_2019_statcap.loc[df_2019_statcap['countrycode'] == country_at_percentile, 'ln_y'].values[0] + 0.3, \n",
    "                         df_2019_statcap.loc[df_2019_statcap['countrycode'] == country_at_percentile, 'statcap'].values[0] + 0.05),\n",
    "                 arrowprops = dict(color = 'black', arrowstyle = '->'))\n",
    "\n",
    "#Final-----------------------------------\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, I will recalculate while removing outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No Outliers-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#Identifying outliers--------------------\n",
    "Q1 = df_2019_statcap[['ln_y', 'statcap']].quantile(0.25)\n",
    "Q3 = df_2019_statcap[['ln_y', 'statcap']].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = ((df_2019_statcap[['ln_y', 'statcap']] < (Q1 - 1.5 * IQR)) | (df_2019_statcap[['ln_y', 'statcap']] > (Q3 + 1.5 * IQR))).any(axis = 1) \n",
    "no_outliers_statcap = df_2019_statcap[~outliers]\n",
    "\n",
    "#Scatterplot-----------------------------\n",
    "plt.scatter(no_outliers_statcap['ln_y'], no_outliers_statcap['statcap'])\n",
    "plt.xlabel(\"ln(y) (natural logarithm of income per worker)\")\n",
    "plt.ylabel(\"statcap (statistical capacity indicator)\")\n",
    "plt.title(\"Scatterplot for statcap against ln(y) (No outliers)\")\n",
    "\n",
    "#Regression line------------------------\n",
    "reg_statcap_coefficients_no_outliers = np.polyfit(no_outliers_statcap['ln_y'], no_outliers_statcap['statcap'], 1)\n",
    "x_vals_no_outliers = no_outliers_statcap['ln_y']\n",
    "y_vals_no_outliers = reg_statcap_coefficients_no_outliers[0] * x_vals_no_outliers + reg_statcap_coefficients_no_outliers[1]\n",
    "sign = \"+\" if reg_statcap_coefficients_no_outliers[1] >= 0 else \"-\"\n",
    "plt.plot(x_vals_no_outliers, y_vals_no_outliers, color = 'xkcd:blood', label = f\"statcap = {reg_statcap_coefficients_no_outliers[0]:.4f}ln(y) {sign} {abs(reg_statcap_coefficients_no_outliers[1]):.2f}\")\n",
    "\n",
    "#Percentiles-----------------------------\n",
    "for percentile in percentiles:\n",
    "    statcap_percentile_value = np.percentile(no_outliers_statcap['statcap'], percentile) \n",
    "    \n",
    "    closest_country_idx = np.abs(no_outliers_statcap['statcap'] - statcap_percentile_value).idxmin() \n",
    "    country_at_percentile = no_outliers_statcap.loc[closest_country_idx, 'countrycode']\n",
    "    \n",
    "    plt.text(no_outliers_statcap.loc[no_outliers_statcap['countrycode'] == country_at_percentile, 'ln_y'],\n",
    "            no_outliers_statcap.loc[no_outliers_statcap['countrycode'] == country_at_percentile, 'statcap'],\n",
    "             f'{country_at_percentile}\\n{percentile}th', fontsize = 9, ha = 'right') \n",
    "    plt.annotate(\"\", \n",
    "                 xy = (no_outliers_statcap.loc[no_outliers_statcap['countrycode'] == country_at_percentile, 'ln_y'].values[0],\n",
    "                     no_outliers_statcap.loc[no_outliers_statcap['countrycode'] == country_at_percentile, 'statcap'].values[0]),\n",
    "                 xytext = (no_outliers_statcap.loc[no_outliers_statcap['countrycode'] == country_at_percentile, 'ln_y'].values[0] + 0.2, \n",
    "                         no_outliers_statcap.loc[no_outliers_statcap['countrycode'] == country_at_percentile, 'statcap'].values[0] + 0.025),\n",
    "                 arrowprops = dict(color = 'black', arrowstyle = '->'))\n",
    "\n",
    "#Final----------------------------------\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "var_statcap = np.var(df_2019_statcap['statcap'])\n",
    "explained_statcap = (var_statcap**(1 - alpha))\n",
    "share_statcap = (explained_statcap / var_y) * 100\n",
    "print(f\"The share of variance in y explained by statcap is {share_statcap}%.\")\n",
    "\n",
    "var_statcap_no_outliers = np.var(no_outliers_statcap['statcap'])\n",
    "explained_statcap_no_outliers = (var_statcap_no_outliers**(1 - alpha))\n",
    "share_statcap_no_outliers = (explained_statcap_no_outliers / var_y) * 100\n",
    "print(f\"The share of variance in y explained by statcap is {share_statcap_no_outliers}% (no outliers).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the variance in statcap only explains a small overall percentage of the variance in y, this share is greater than that of A and h. Thus, it's an important variable to analyse further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several possible ways to model institutional quality into the existing model. In this context, I look to the Institutions-augmented Solow model, as documented by, among other sources, Tebaldi & Mohan (2009). They modify the Solow model to include institutional quality, through a variable $T$ that takes a value between 0 (worst institutions) and 1 (best institutions), and thus create a baseline model:\n",
    "\n",
    "$Y = K^{\\alpha T} (AL)^{(1 - \\alpha T)}$\n",
    "\n",
    "They then introduce an extended model: \n",
    "\n",
    "$Y = A^{(T - 1)} K^{\\alpha T} (AL)^{(1 - \\alpha T)}$\n",
    "\n",
    "I will draw on this to incorporate it into the model we have above. Similar to this model, I will incorporate institutional quality $T$ into capital per worker and total factor productivity:\n",
    "\n",
    "$y = A^{(T - 1)} k^{\\alpha T} (avh \\cdot h)^{(1-\\alpha)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in Caselli, I will produce a scatterplot between statcap and $success_1$ and $success_2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outliers--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#Define variables and measure of success\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/jivizcaino/PWT10.1/main/pwt101.csv') #Reinitialise df, just in case\n",
    "df_2019 = df[df['year'] == 2019]\n",
    "alpha = 1 / 3\n",
    "k = df_2019['rnna'] / df_2019['emp']\n",
    "y_kh = k**alpha * (avh * h)**(1 - alpha)\n",
    "y = df_2019['rgdpna']\n",
    "statcap = df_2019['statcap']\n",
    "success_1 =  y_kh / y\n",
    "\n",
    "#Filters out success_1 observations with no corresponding statcap observation\n",
    "filtered_success_1 = success_1[df_2019.index.isin(df_2019['statcap'].index)]\n",
    "\n",
    "#Drops NaN values from both statcap and filtered_success_1\n",
    "data_for_regression = pd.DataFrame({'statcap': statcap[df_2019.index.isin(statcap.index)], 'success_1': filtered_success_1}) #Creates dataframe with statcap and success_1, containing \n",
    "#filtered values\n",
    "data_for_regression = data_for_regression.dropna()\n",
    "\n",
    "#Scatterplot-----------------------------\n",
    "plt.scatter(data_for_regression['statcap'], data_for_regression['success_1'])\n",
    "plt.xlabel(\"statcap (statistical capacity indicator)\")\n",
    "plt.ylabel(\"$success_1$\")\n",
    "plt.title(\"Scatterplot for $success_1$ against statcap\")\n",
    "\n",
    "#Regression line------------------------\n",
    "reg_success_1_coefficients = np.polyfit(data_for_regression['statcap'], data_for_regression['success_1'], 1)\n",
    "x_vals_success_1 = np.linspace(data_for_regression['statcap'].min(), data_for_regression['statcap'].max(), 100)\n",
    "y_vals_success_1 = reg_success_1_coefficients[0] * x_vals_success_1 + reg_success_1_coefficients[1]\n",
    "sign_success_1 = \"+\" if reg_success_1_coefficients[1] >= 0 else \"-\"\n",
    "plt.plot(x_vals_success_1, y_vals_success_1, color = 'red', label = f\"$success_1$ = {reg_success_1_coefficients[0]:.4f}statcap {sign_success_1} {abs(reg_success_1_coefficients[1]):.4f}\")\n",
    "\n",
    "#Final-----------------------------------\n",
    "plt.legend(loc = 'upper right') #Legend was in the centre of the diagram otherwise, for some reason\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't seem to produce much correlation. However, when outliers are removed, there is some positive correlation, though not very strong:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No Outliers-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#Identifying outliers--------------------\n",
    "Q1 = data_for_regression[['success_1', 'statcap']].quantile(0.25)\n",
    "Q3 = data_for_regression[['success_1', 'statcap']].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = ((data_for_regression[['success_1', 'statcap']] < (Q1 - 1.5 * IQR)) | (data_for_regression[['success_1', 'statcap']] > (Q3 + 1.5 * IQR))).any(axis = 1)\n",
    "no_outliers_df = data_for_regression[~outliers]\n",
    "filtered_success_1 = success_1[no_outliers_df.index]\n",
    "\n",
    "data_for_regression = pd.DataFrame({'statcap': no_outliers_df['statcap'], 'success_1': filtered_success_1})\n",
    "data_for_regression = data_for_regression.dropna()\n",
    "\n",
    "#Scatterplot-----------------------------\n",
    "plt.scatter(data_for_regression['statcap'], data_for_regression['success_1'])\n",
    "plt.xlabel(\"statcap (statistical capacity indicator)\")\n",
    "plt.ylabel(\"$success_1$\")\n",
    "plt.title(\"Scatterplot for $success_1$ against statcap\")\n",
    "\n",
    "#Regression line------------------------\n",
    "reg_success_1_coefficients = np.polyfit(data_for_regression['statcap'], data_for_regression['success_1'], 1)\n",
    "x_vals_success_1 = np.linspace(data_for_regression['statcap'].min(), data_for_regression['statcap'].max(), 100)\n",
    "y_vals_success_1 = reg_success_1_coefficients[0] * x_vals_success_1 + reg_success_1_coefficients[1]\n",
    "sign_success_1 = \"+\" if reg_success_1_coefficients[1] >= 0 else \"-\"\n",
    "plt.plot(x_vals_success_1, y_vals_success_1, color = 'xkcd:blood', label = f\"$success_1$ = {reg_success_1_coefficients[0]:.4f}statcap {sign_success_1} {abs(reg_success_1_coefficients[1]):.4f}\")\n",
    "\n",
    "#Final-----------------------------------\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will calculate $success_2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outliers--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#Define variable and measure of success\n",
    "A_tilde = y_kh / y\n",
    "success_2 = A_tilde / y\n",
    "\n",
    "#Filters out success_2 observations with no corresponding statcap observation (above method did not work)\n",
    "filtered_success_2 = success_2[df_2019['statcap'].notna()]\n",
    "\n",
    "data_for_regression = pd.DataFrame({'statcap': statcap, 'success_2': filtered_success_2}).dropna()\n",
    "\n",
    "#Scatterplot-----------------------------\n",
    "plt.scatter(data_for_regression['statcap'], data_for_regression['success_2'])\n",
    "plt.xlabel(\"statcap (statistical capacity indicator)\")\n",
    "plt.ylabel(\"$success_2$\")\n",
    "plt.title(\"Scatterplot for $success_2$ against statcap\")\n",
    "\n",
    "#Regression line------------------------\n",
    "reg_success_2_coefficients = np.polyfit(data_for_regression['statcap'], data_for_regression['success_2'], 1)\n",
    "x_vals_success_2 = np.linspace(data_for_regression['statcap'].min(), data_for_regression['statcap'].max(), 100)\n",
    "y_vals_success_2 = reg_success_2_coefficients[0] * x_vals_success_2 + reg_success_2_coefficients[1]\n",
    "sign_success_2 = \"+\" if reg_success_2_coefficients[1] >= 0 else \"-\"\n",
    "plt.plot(x_vals_success_2, y_vals_success_2, color = 'red', label = f\"$success_2$ = {reg_success_2_coefficients[0]:.7f}statcap {sign_success_2} {abs(reg_success_2_coefficients[1]):.7f}\")\n",
    "\n",
    "#Final-----------------------------------\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, this is an extremely flat line - when outliers are included, there is basically no improvement in efficiency as statcap increases. This is also true when outliers are removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No outliers----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#Identifying outliers--------------------\n",
    "Q1 = data_for_regression[['success_2', 'statcap']].quantile(0.25)\n",
    "Q3 = data_for_regression[['success_2', 'statcap']].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = ((data_for_regression[['success_2', 'statcap']] < (Q1 - 1.5 * IQR)) | (data_for_regression[['success_2', 'statcap']] > (Q3 + 1.5 * IQR))).any(axis = 1)\n",
    "no_outliers_df = data_for_regression[~outliers]\n",
    "\n",
    "filtered_success_2 = success_2[df_2019['statcap'].notna()]\n",
    "\n",
    "data_for_regression = pd.DataFrame({'statcap': no_outliers_df['statcap'], 'success_2': filtered_success_2})\n",
    "data_for_regression = data_for_regression.dropna()\n",
    "\n",
    "#Scatterplot-----------------------------\n",
    "plt.scatter(data_for_regression['statcap'], data_for_regression['success_2'])\n",
    "plt.xlabel(\"statcap (statistical capacity indicator)\")\n",
    "plt.ylabel(\"$success_2$\")\n",
    "plt.title(\"Scatterplot for $success_2$ against statcap\")\n",
    "\n",
    "#Regression line------------------------\n",
    "reg_success_2_coefficients = np.polyfit(data_for_regression['statcap'], data_for_regression['success_2'], 1)\n",
    "x_vals_success_2 = np.linspace(data_for_regression['statcap'].min(), data_for_regression['statcap'].max(), 100)\n",
    "y_vals_success_2 = reg_success_2_coefficients[0] * x_vals_success_2 + reg_success_2_coefficients[1]\n",
    "sign_success_2 = \"+\" if reg_success_2_coefficients[1] >= 0 else \"-\"\n",
    "plt.plot(x_vals_success_2, y_vals_success_2, color = 'xkcd:blood', label = f\"$success_2$ = {reg_success_2_coefficients[0]:.7f}statcap {sign_success_2} {abs(reg_success_2_coefficients[1]):.7f}\")\n",
    "\n",
    "#Final-----------------------------------\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, it seems that if institutions are having any effect, it's through benefits to capital accumulation. However, the strong positive correlation with ln(y) should be noted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few issues with this analysis. First, statistical capacity is by no means a perfect approximation of institutional quality; the structure of the dataset means that there is no direct data on institutional quality, and, by its nature, institutional quality is difficult to measure (Samadi & Alipourian (2021) document 70+ measures of institutional quality!) Second, there is only data on statistical capacity for developing countries, meaning that important countries (i.e. those that have developed successfully) are left out of the dataset; only 124 out of the 183 countries are in this sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The number of countries with data on statistical capacity is {len(df_2019_statcap['statcap'])}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, development accounting, as Caselli puts it, investigates proximate causes of growth, not fundamental causes, as is institutional quality. Perhaps most importantly, there is an issue of reverse causality. It's very much possible that an increase in income per capita increases institutional quality/statistical capacity, and not the other way around - or perhaps even a third factor influencing both income per capita and institutional quality. Existing research theorises that it is institutional quality that has an impact on GDP per capita (Acemoglu, Johnson & Robinson, 2005), and thus I believe that more research should be conducted into how institutions augment the income per worker function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. What conclusion can you draw from the exercise conducted? Are there any policy recommendations that emerge from the exercise? Are there any avenues for research that come out of the exercise?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exercise above aims to update Caselli's analysis of why different countries have different income levels. Firstly, it's clear to see that there's huge variation in countries' incomes; Ireland has an income per worker $208,583 higher than Venezuela's. Thus, it is pertinent to examine what drives these differences in incomes.\n",
    "\n",
    "I produce scatterplots, and show that log of income per worker is positively correlated with the log of physical and human capital, negatively correlated with average hours worked, and has no strong correlation, positive or negative, with share of labour compensation in GDP and log of total factor productivity.\n",
    "\n",
    "Next, I compute several measures of success to see how well some factors explain the variation in income per worker across countries. I find that capital and average hours worked account for most of the variance across countries, while the largest measure of success is $success_2$. Thus, it seems that capital and efficiency are most important for income per worker. \n",
    "\n",
    "Then, I calculate how much variation in $ln(y_{kh})$ is explained by $ln(k)$, $ln(avh * h)$, and the covariance between these two, for different values of ùõº. I find that no matter ùõº's value, $ln(k)$ has the largest impact on the variation in $ln(y_{kh})$. This shows that for $ln(y_{kh})$ specifically, $ln(k)$ is the most important factor, which again supports my findings for the variation in $y$ ($ln(k)$ was the most important factor beyond efficiency). \n",
    "\n",
    "Next, I find that efficiency is most important in $y$, rather than the specific factors of production, as $success_2$ is greater than $success_1$. \n",
    "\n",
    "I then decide to investigate whether $statcap$ explains differences in income per worker. I find a clear positive correlation between $statcap$ and $ln(y)$, prompting me to investigate further. I find that, generally, $statcap$ is not correlated with $success_1$ or $success_2$, but I think some more research into this area, including how to augment the production function to include institutional quality beyond the Solow model, is warranted. \n",
    "\n",
    "The obvious result is that capital accumulation and, particularly, efficiency in using capital and other factors, are most important for growth in income per worker. Thus, policy recommendations should be salient on these findings; policy should aim to increase efficiency in the use of capital first and foremost. For example, investment in infrastructure will allow for greater efficiency in allocation of capital, as this would reduce transportation costs and therefore increase efficiency, which would translate over to income per worker. Policy should also be used to encourage capital accumulation - for example, reducing regulation or providing tax incentives for businesses purchasing capital could increase income per worker. \n",
    "\n",
    "As for possible reserach opportunities, as I've stated, I believe further research should be conducted into how institutional quality can be incorporated into the production function, and whether there's a solid link between statstical capacity and institutional quality. Beyond this, it is crucial to delve deeper into average hours worked ($avh$) and its role within the production function. Particular attention should be paid to the direction of causality; does decreasing average hours worked result in more efficient employees, thus increasing income per worker? Or, alternatively, does an increase in income per worker result in better working conditions, or better unionisation, thus leading to less average hours worked? This is a very interesting question, and its answer may have implications for the production function posited here - if increases in income per worker cause average hours worked to decrease, then there's an argument that $avh$ should not be included in the production function at all. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Caselli (2003). *Accounting for Cross-Country Income Differences*. NBER Working Paper Series, 10828.\n",
    "2. Nawaz, Iqbal & Khan (2014). *The Impact of Institutional Quality on Economic Growth: Panel Evidence*. The Pakistan Development Review, 53(1): 15-31.\n",
    "3. Tebaldi & Mohan (2009). *Institutions-augmented solow model and income clubs*. A Economia em Revista, 17(2): 5-13.\n",
    "4. Acemoglu, Johnson & Robinson (2005). *Institutions as a Fundamental Cause of Long-Run Growth*. Handbook of Economic Growth, 1(6): 385-472.\n",
    "5. Samadi & Alipourian (2021). *Measuring Institutional Quality: A Review.* Dynamics of Institutional Change in Emerging Market Economies: Theories, Concepts and Mechanisms: 143-171."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
